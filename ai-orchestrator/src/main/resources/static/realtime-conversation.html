<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>实时语音对话体验</title>
    <style>
        :root {
            color-scheme: light dark;
            font-family: "Segoe UI", "Helvetica Neue", Arial, "Noto Sans", "PingFang SC", "Hiragino Sans GB", sans-serif;
            background-color: #030712;
            color: #e6edf3;
        }
        body {
            margin: 0;
            min-height: 100vh;
            padding: 2.5rem 1.25rem 3rem;
            display: flex;
            justify-content: center;
        }
        .card {
            width: min(960px, 100%);
            background: rgba(13, 17, 23, 0.9);
            border: 1px solid rgba(255, 255, 255, 0.08);
            border-radius: 18px;
            padding: 2.75rem 2.5rem;
            box-shadow: 0 24px 48px rgba(15, 23, 42, 0.55);
            backdrop-filter: blur(16px);
        }
        h1 {
            font-size: clamp(1.85rem, 1.2rem + 1.8vw, 2.8rem);
            margin: 0 0 1.75rem;
        }
        p {
            margin: 0 0 1.5rem;
            line-height: 1.7;
            color: rgba(226, 232, 240, 0.8);
        }
        .nav-link {
            display: inline-flex;
            align-items: center;
            gap: 0.35rem;
            margin-bottom: 1.5rem;
            color: rgba(148, 163, 184, 0.85);
            text-decoration: none;
        }
        .nav-link::before {
            content: "←";
        }
        .controls {
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
            align-items: center;
            margin-bottom: 1.75rem;
        }
        label.inline {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
            font-weight: 600;
        }
        input[type="number"] {
            width: 160px;
            padding: 0.55rem 0.75rem;
            border-radius: 10px;
            border: 1px solid rgba(148, 163, 184, 0.4);
            background: rgba(2, 6, 23, 0.6);
            color: inherit;
            font-size: 0.95rem;
        }
        button.primary {
            padding: 0.85rem 1.8rem;
            border-radius: 999px;
            border: none;
            font-weight: 600;
            letter-spacing: 0.05em;
            background: linear-gradient(135deg, #34d399, #22d3ee);
            color: #02131f;
            cursor: pointer;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }
        button.primary:hover {
            transform: translateY(-1px);
            box-shadow: 0 12px 28px rgba(45, 212, 191, 0.32);
        }
        button.primary:disabled {
            opacity: 0.55;
            cursor: not-allowed;
            box-shadow: none;
        }
        button.danger {
            padding: 0.85rem 1.8rem;
            border-radius: 999px;
            border: 1px solid rgba(248, 113, 113, 0.65);
            font-weight: 600;
            background: transparent;
            color: #fca5a5;
            cursor: pointer;
            transition: background 0.2s ease, color 0.2s ease;
        }
        button.danger:hover {
            background: rgba(248, 113, 113, 0.12);
            color: #fecaca;
        }
        button.danger:disabled {
            opacity: 0.45;
            cursor: not-allowed;
        }
        .status {
            padding: 0.75rem 1rem;
            border-radius: 12px;
            background: rgba(30, 64, 175, 0.18);
            border: 1px solid rgba(96, 165, 250, 0.25);
            font-size: 0.95rem;
            color: rgba(191, 219, 254, 0.9);
        }
        .panels {
            display: grid;
            gap: 1.5rem;
        }
        .panel {
            padding: 1.5rem;
            border-radius: 14px;
            background: rgba(2, 6, 23, 0.7);
            border: 1px solid rgba(59, 130, 246, 0.2);
        }
        .panel h2 {
            margin: 0 0 1rem;
            font-size: 1.15rem;
        }
        .transcript, .reply {
            display: flex;
            flex-direction: column;
            gap: 0.8rem;
        }
        .bubble {
            padding: 0.75rem 1rem;
            border-radius: 12px;
            background: rgba(15, 23, 42, 0.55);
            border: 1px solid rgba(148, 163, 184, 0.2);
            line-height: 1.6;
        }
        .bubble.user {
            border-color: rgba(59, 130, 246, 0.45);
            background: rgba(37, 99, 235, 0.12);
        }
        .bubble.assistant {
            border-color: rgba(16, 185, 129, 0.45);
            background: rgba(16, 185, 129, 0.12);
        }
        .log {
            max-height: 180px;
            overflow-y: auto;
            font-family: "Fira Code", "SFMono-Regular", "Menlo", monospace;
            font-size: 0.85rem;
            line-height: 1.55;
            white-space: pre-wrap;
            color: rgba(203, 213, 225, 0.85);
        }
        .hint {
            margin-top: 1rem;
            font-size: 0.85rem;
            color: rgba(148, 163, 184, 0.75);
        }
    </style>
</head>
<body>
<div class="card">
    <a class="nav-link" href="index.html">返回首页</a>
    <h1>实时语音对话</h1>
    <p>点击“开始体验”后，浏览器会请求麦克风权限并将 PCM16LE 音频通过 WebSocket 接口 <code>/ws/conversation</code> 流式发送。静默 2 秒触发 VAD 后，服务端完成 ASR → LLM → TTS 全链路流式处理，前端立即展示增量文本并播放返回的语音流。</p>
    <div class="controls">
        <label class="inline">STT 目标采样率
            <input id="stt-rate" type="number" value="16000" min="8000" max="48000" step="1000">
        </label>
        <button id="start" class="primary">开始体验</button>
        <button id="stop" class="danger" disabled>结束体验</button>
        <div id="status" class="status">未连接</div>
    </div>
    <div class="panels">
        <section class="panel">
            <h2>识别文本</h2>
            <div id="transcript" class="transcript"></div>
        </section>
        <section class="panel">
            <h2>LLM 回复</h2>
            <div id="reply" class="reply"></div>
            <p class="hint">若无音频回复，可检查 Kokoro TTS 服务是否正常。手动点击“结束体验”可立即停止采集。</p>
        </section>
        <section class="panel">
            <h2>调试日志</h2>
            <div id="log" class="log"></div>
        </section>
    </div>
</div>
<script>
    const startBtn = document.getElementById('start');
    const stopBtn = document.getElementById('stop');
    const statusEl = document.getElementById('status');
    const transcriptEl = document.getElementById('transcript');
    const replyEl = document.getElementById('reply');
    const logEl = document.getElementById('log');
    const sttRateInput = document.getElementById('stt-rate');

    const vadSilenceMs = 2000;
    const vadThreshold = 0.015;

    let mediaStream = null;
    let captureContext = null;
    let processor = null;
    let playbackContext = null;
    let playbackTime = 0;
    let playbackQueue = Promise.resolve();
    let currentUserBubble = null;
    let currentAssistantBubble = null;

    let resampleBuffer = [];
    let sampleRateRatio = 1;
    let recordingActive = false;
    let awaitingReply = false;
    let lastVoiceTime = 0;
    let hasSentAudio = false;

    let conversationHistory = [];

    let socket = null;
    let socketReady = false;
    let pendingReadyResolve = null;
    let pendingReadyReject = null;

    startBtn.addEventListener('click', startExperience);
    stopBtn.addEventListener('click', stopExperience);

    function log(message) {
        const time = new Date().toLocaleTimeString();
        logEl.textContent += `[${time}] ${message}\n`;
        logEl.scrollTop = logEl.scrollHeight;
    }

    function setStatus(text) {
        statusEl.textContent = text;
    }

    function resetUi() {
        transcriptEl.innerHTML = '';
        replyEl.innerHTML = '';
        logEl.textContent = '';
        conversationHistory = [];
        playbackTime = 0;
        playbackQueue = Promise.resolve();
        currentUserBubble = null;
        currentAssistantBubble = null;
    }

    async function startExperience() {
        if (recordingActive || awaitingReply) {
            return;
        }
        resetUi();
        resampleBuffer = [];
        sampleRateRatio = 1;
        hasSentAudio = false;
        startBtn.disabled = true;
        sttRateInput.disabled = true;
        stopBtn.disabled = false;
        setStatus('连接服务中...');
        try {
            await connectWebSocket();
            sendStartMessage();
            await setupMicrophone();
            recordingActive = true;
            awaitingReply = false;
            lastVoiceTime = performance.now();
            setStatus('请开始讲话');
            log('麦克风初始化完成，等待语音输入');
        } catch (error) {
            log(`初始化失败：${error.message}`);
            setStatus('初始化失败');
            await stopExperience();
        }
    }

    async function stopExperience() {
        recordingActive = false;
        awaitingReply = false;
        resampleBuffer = [];
        hasSentAudio = false;
        startBtn.disabled = false;
        sttRateInput.disabled = false;
        stopBtn.disabled = true;
        if (processor) {
            try { processor.disconnect(); } catch (_) {}
            processor.onaudioprocess = null;
            processor = null;
        }
        if (captureContext) {
            try { captureContext.close(); } catch (_) {}
            captureContext = null;
        }
        if (mediaStream) {
            mediaStream.getTracks().forEach(track => track.stop());
            mediaStream = null;
        }
        if (socket && socket.readyState === WebSocket.OPEN) {
            try {
                socket.send(JSON.stringify({ type: 'reset_history' }));
            } catch (_) {}
            socket.close();
        }
        socketReady = false;
        if (playbackContext) {
            try { playbackContext.close(); } catch (_) {}
            playbackContext = null;
        }
        playbackTime = 0;
        playbackQueue = Promise.resolve();
        setStatus('未连接');
        log('对话已结束');
    }

    async function connectWebSocket() {
        if (socket && socket.readyState === WebSocket.OPEN && socketReady) {
            return;
        }
        if (socket && socket.readyState !== WebSocket.CLOSED && socket.readyState !== WebSocket.CLOSING) {
            socket.close();
        }
        const url = `${window.location.protocol === 'https:' ? 'wss' : 'ws'}://${window.location.host}/ws/conversation`;
        socket = new WebSocket(url);
        socketReady = false;
        socket.addEventListener('message', handleSocketMessage);
        socket.addEventListener('open', () => {
            log('WebSocket 连接已建立');
        });
        socket.addEventListener('close', (event) => {
            socketReady = false;
            log(`WebSocket 已关闭 (code=${event.code})`);
            if (pendingReadyReject) {
                pendingReadyReject(new Error('WebSocket 连接已关闭'));
            }
            pendingReadyResolve = null;
            pendingReadyReject = null;
            if (recordingActive || awaitingReply) {
                setStatus('连接已断开');
            }
        });
        socket.addEventListener('error', () => {
            log('WebSocket 发生错误');
            if (pendingReadyReject) {
                pendingReadyReject(new Error('WebSocket 连接错误'));
            }
            pendingReadyResolve = null;
            pendingReadyReject = null;
        });
        await new Promise((resolve, reject) => {
            pendingReadyResolve = resolve;
            pendingReadyReject = reject;
        });
    }

    function sendStartMessage() {
        if (!socket || socket.readyState !== WebSocket.OPEN) {
            throw new Error('WebSocket 未就绪');
        }
        const payload = {
            type: 'start',
            sampleRate: targetSampleRate(),
            channels: 1,
            bitDepth: 16,
            history: conversationHistory
        };
        socket.send(JSON.stringify(payload));
        log('已发送 start 消息');
    }

    async function setupMicrophone() {
        mediaStream = await navigator.mediaDevices.getUserMedia({
            audio: { channelCount: 1, echoCancellation: false, noiseSuppression: false, autoGainControl: false }
        });
        captureContext = new (window.AudioContext || window.webkitAudioContext)();
        await captureContext.resume();
        const source = captureContext.createMediaStreamSource(mediaStream);
        processor = captureContext.createScriptProcessor(4096, 1, 1);
        sampleRateRatio = captureContext.sampleRate / targetSampleRate();
        resampleBuffer = [];

        processor.onaudioprocess = (event) => {
            if (!recordingActive || awaitingReply) {
                return;
            }
            const input = event.inputBuffer.getChannelData(0);
            collectSamples(input);
        };

        const silentGain = captureContext.createGain();
        silentGain.gain.value = 0;
        source.connect(processor);
        processor.connect(silentGain);
        silentGain.connect(captureContext.destination);
        log(`麦克风已打开（设备采样率 ${Math.round(captureContext.sampleRate)}Hz）`);
    }

    function targetSampleRate() {
        const value = Number(sttRateInput.value);
        return Number.isFinite(value) && value > 0 ? value : 16000;
    }

    function collectSamples(chunk) {
        resampleBuffer.push(...chunk);
        if (!Number.isFinite(sampleRateRatio) || sampleRateRatio <= 0) {
            sampleRateRatio = 1;
        }
        const available = Math.floor(resampleBuffer.length / sampleRateRatio);
        if (available === 0) {
            return;
        }
        const output = new Int16Array(available);
        let energy = 0;
        for (let i = 0; i < available; i++) {
            const position = i * sampleRateRatio;
            const left = Math.floor(position);
            const right = Math.min(left + 1, resampleBuffer.length - 1);
            const frac = position - left;
            const sample = resampleBuffer[left] + (resampleBuffer[right] - resampleBuffer[left]) * frac;
            const clipped = Math.max(-1, Math.min(1, sample));
            const intSample = Math.round(clipped * 0x7fff);
            output[i] = intSample;
            const normalized = intSample / 0x7fff;
            energy += normalized * normalized;
        }
        const consumed = Math.floor(available * sampleRateRatio);
        resampleBuffer = resampleBuffer.slice(consumed);
        const meanSquare = output.length === 0 ? 0 : energy / output.length;
        if (output.length > 0) {
            sendAudioChunk(output);
        }
        handleVad(meanSquare);
    }

    function sendAudioChunk(int16) {
        if (!socketReady || !socket || socket.readyState !== WebSocket.OPEN) {
            return;
        }
        const chunkBase64 = encodePcm16ToBase64(int16);
        socket.send(JSON.stringify({ type: 'audio', chunk: chunkBase64 }));
        hasSentAudio = true;
    }

    function handleVad(meanSquare) {
        if (!recordingActive || awaitingReply) {
            return;
        }
        if (meanSquare > vadThreshold * vadThreshold) {
            lastVoiceTime = performance.now();
            setStatus('检测到语音输入');
        } else if (hasSentAudio) {
            const elapsed = performance.now() - lastVoiceTime;
            if (elapsed > vadSilenceMs) {
                finalizeSegment();
            }
        }
    }

    function finalizeSegment() {
        if (awaitingReply) {
            return;
        }
        awaitingReply = true;
        recordingActive = false;
        resampleBuffer = [];
        setStatus('等待模型回复...');
        log('检测到语音段落结束，发送 stop 信号');
        if (socket && socket.readyState === WebSocket.OPEN) {
            socket.send(JSON.stringify({ type: 'stop' }));
        }
        hasSentAudio = false;
    }

    function handleSocketMessage(event) {
        let data;
        try {
            data = JSON.parse(event.data);
        } catch (error) {
            log(`无法解析服务端消息：${error}`);
            return;
        }
        switch (data.type) {
            case 'ready':
                socketReady = true;
                log('服务端会话已就绪');
                setStatus('服务已连接');
                if (pendingReadyResolve) {
                    pendingReadyResolve();
                }
                pendingReadyResolve = null;
                pendingReadyReject = null;
                break;
            case 'listening':
                log('服务端准备接收音频');
                setStatus('请开始讲话');
                break;
            case 'transcript':
                if (data.text) {
                    updateUserTranscript(data.text, Boolean(data.final));
                    if (data.final) {
                        log(`识别完成：${data.text}`);
                    }
                }
                break;
            case 'assistant_text':
                if (data.text) {
                    updateAssistantReply(data.text);
                }
                break;
            case 'tts_chunk':
                if (data.audioBase64) {
                    queuePlaybackStep(() => schedulePlaybackChunk(data));
                }
                break;
            case 'debug':
                logDebugMessage(data);
                break;
            case 'tts_complete':
                if (Array.isArray(data.history)) {
                    conversationHistory = data.history;
                }
                queuePlaybackStep(async () => {
                    const lastEntry = Array.isArray(data.history) ? data.history[data.history.length - 1] : null;
                    if (lastEntry && lastEntry.role === 'assistant' && lastEntry.content) {
                        log(`助手回复：${lastEntry.content}`);
                    }
                    awaitingReply = false;
                    currentAssistantBubble = null;
                    lastVoiceTime = performance.now();
                    setStatus('请继续讲话');
                    try {
                        sendStartMessage();
                        hasSentAudio = false;
                        recordingActive = true;
                    } catch (error) {
                        log(`重新开始会话失败：${error.message || error}`);
                        setStatus('无法继续对话，请手动重新开始');
                    }
                });
                break;
            case 'no_speech':
                awaitingReply = false;
                recordingActive = false;
                lastVoiceTime = performance.now();
                currentUserBubble = null;
                setStatus('未识别到语音，请继续讲话');
                log('ASR 未识别到内容');
                try {
                    sendStartMessage();
                    hasSentAudio = false;
                    recordingActive = true;
                } catch (error) {
                    log(`重新开始会话失败：${error.message || error}`);
                    setStatus('无法继续对话，请手动重新开始');
                }
                break;
            case 'error':
                log(`服务端错误：${data.code || ''} ${data.message || ''}`.trim());
                break;
            default:
                log(`收到未知类型消息：${data.type}`);
                break;
        }
    }

    function appendBubble(container, role, text) {
        const div = document.createElement('div');
        div.className = `bubble ${role}`;
        div.textContent = text;
        container.appendChild(div);
        container.scrollTop = container.scrollHeight;
        return div;
    }

    function updateUserTranscript(text, isFinal) {
        if (!currentUserBubble) {
            currentUserBubble = appendBubble(transcriptEl, 'user', text);
        } else {
            currentUserBubble.textContent = text;
        }
        if (isFinal) {
            currentUserBubble = null;
        }
    }

    function updateAssistantReply(delta) {
        if (!currentAssistantBubble) {
            currentAssistantBubble = appendBubble(replyEl, 'assistant', delta);
        } else {
            currentAssistantBubble.textContent += delta;
        }
        replyEl.scrollTop = replyEl.scrollHeight;
    }

    function queuePlaybackStep(step) {
        playbackQueue = playbackQueue
            .then(() => step())
            .catch(error => {
                log(`播放队列错误：${error.message || error}`);
            });
    }

    function logDebugMessage(data) {
        if (!data || typeof data !== 'object') {
            return;
        }
        const stage = typeof data.stage === 'string' ? data.stage.toUpperCase() : 'DEBUG';
        const status = typeof data.status === 'string' ? data.status.toUpperCase() : '';
        let message = status ? `${stage} ${status}` : stage;
        if (data.message) {
            message += ` - ${data.message}`;
        }
        const timing = [];
        if (data.startTime) {
            timing.push(`start=${formatDebugTime(data.startTime)}`);
        }
        if (data.endTime) {
            timing.push(`end=${formatDebugTime(data.endTime)}`);
        }
        const duration = Number(data.durationMs);
        if (Number.isFinite(duration) && duration >= 0) {
            timing.push(`duration=${Math.round(duration)}ms`);
        }
        if (timing.length > 0) {
            message += ` [${timing.join(', ')}]`;
        }
        const usageInfo = extractTokenUsage(data.extra);
        if (usageInfo) {
            message += ` | tokens: ${usageInfo}`;
        }
        if (data.extra) {
            try {
                message += ` | ${JSON.stringify(data.extra)}`;
            } catch (_) {
                message += ' | extra:<unserializable>';
            }
        }
        log(message);
    }

    function formatDebugTime(isoString) {
        const date = new Date(isoString);
        if (Number.isNaN(date.getTime())) {
            return isoString;
        }
        const time = date.toLocaleTimeString();
        const millis = String(date.getMilliseconds()).padStart(3, '0');
        return `${time}.${millis}`;
    }

    function extractTokenUsage(extra) {
        if (!extra || typeof extra !== 'object') {
            return '';
        }
        const usage = extra.usage;
        if (!usage || typeof usage !== 'object') {
            return '';
        }
        const parts = [];
        const prompt = Number(usage.prompt_tokens);
        if (Number.isFinite(prompt)) {
            parts.push(`prompt=${prompt}`);
        }
        const completion = Number(usage.completion_tokens);
        if (Number.isFinite(completion)) {
            parts.push(`completion=${completion}`);
        }
        const total = Number(usage.total_tokens);
        if (Number.isFinite(total)) {
            parts.push(`total=${total}`);
        }
        return parts.join(', ');
    }

    function schedulePlaybackChunk(message) {
        const sr = Number(message.sampleRate) || targetSampleRate();
        const channels = Math.max(1, Number(message.channels) || 1);
        const pcm = decodeBase64ToInt16(message.audioBase64);
        if (pcm.length === 0) {
            return Promise.resolve();
        }
        return playInt16Buffer(pcm, sr, channels);
    }

    async function playInt16Buffer(int16, sampleRate, channels) {
        if (int16.length === 0) {
            return;
        }
        const sr = Math.max(1, sampleRate || targetSampleRate());
        const ch = Math.max(1, channels || 1);
        if (!playbackContext) {
            playbackContext = new (window.AudioContext || window.webkitAudioContext)();
        }
        if (playbackContext.state === 'suspended') {
            await playbackContext.resume();
        }
        const frameCount = Math.floor(int16.length / ch);
        const buffer = playbackContext.createBuffer(ch, frameCount, sr);
        for (let channel = 0; channel < ch; channel++) {
            const channelData = buffer.getChannelData(channel);
            for (let i = 0; i < frameCount; i++) {
                channelData[i] = int16[i * ch + channel] / 0x7fff;
            }
        }
        await new Promise((resolve) => {
            const source = playbackContext.createBufferSource();
            source.buffer = buffer;
            source.connect(playbackContext.destination);
            const startAt = Math.max(playbackContext.currentTime, playbackTime);
            source.start(startAt);
            playbackTime = startAt + buffer.duration;
            source.addEventListener('ended', resolve, { once: true });
        });
    }

    function encodePcm16ToBase64(int16) {
        const uint8 = new Uint8Array(int16.buffer, int16.byteOffset, int16.byteLength);
        let binary = '';
        const chunkSize = 0x8000;
        for (let i = 0; i < uint8.length; i += chunkSize) {
            const chunk = uint8.subarray(i, i + chunkSize);
            binary += String.fromCharCode(...chunk);
        }
        return btoa(binary);
    }

    function decodeBase64ToInt16(base64) {
        const binary = atob(base64);
        const len = binary.length;
        const bytes = new Uint8Array(len);
        for (let i = 0; i < len; i++) {
            bytes[i] = binary.charCodeAt(i);
        }
        return new Int16Array(bytes.buffer);
    }
</script>
</body>
</html>
